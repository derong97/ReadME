---VM Configurations---
• AMI image: ami-0f82752aa17ff8f5d
• Ubuntu 18.04 LTS
• t2.xlarge
• disk space 32GB +
• they must be in the same region, same subnet
• port 22 must be open
• allow all traffic in security group


---Compilation of command lines to set up hadoop/sqoop after ssh---
(assume IP addresses are as follows)
namenode: 3.230.143.188 (Private: 172.31.70.220)
datanode1: 3.237.15.53 (Private: 172.31.68.101)


(all nodes)
sudo adduser hadoop

sudo vim /etc/hosts
(add lines at the top)
172.31.70.220 namenode
172.31.68.101 datanode1

sudo sh -c 'echo "hadoop ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers.d/90-hadoop'
sudo sysctl vm.swappiness=10

sudo vim /etc/ssh/sshd_config
(change "PasswordAuthentication no" to "PasswordAuthentication yes")

sudo service sshd restart


(on namenode)
sudo apt-get install -y ssh
ssh-keygen

ssh-copy-id hadoop@namenode

ssh-copy-id hadoop@datanode1

(all nodes)
sudo apt-get update
sudo apt-get install -y openjdk-8-jdk

sudo su hadoop

(on namenode)
mkdir download
cd download
wget https://apachemirror.sg.wuchna.com/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz

cd ./download
tar zxvf hadoop-3.3.0.tar.gz
export JH="\/usr\/lib\/jvm\/java-8-openjdk-amd64"
sed -i "s/# export JAVA_HOME=.*/export\ JAVA_HOME=${JH}/g" \
hadoop-3.3.0/etc/hadoop/hadoop-env.sh

MASTER=namenode
WORKERS="datanode1"


echo -e "<?xml version=\"1.0\"?>
<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>
<\x21-- Put site-specific property overrides in this file. -->
<configuration>
<property>
<name>fs.defaultFS</name>
<value>hdfs://${MASTER}:9000</value>
</property>
</configuration>
" > hadoop-3.3.0/etc/hadoop/core-site.xml

echo -e "<?xml version=\"1.0\"?>
<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>
<\x21-- Put site-specific property overrides in this file. -->
<configuration>
<property>
<name>dfs.replication</name>
<value>3</value>
</property>
<property>
<name>dfs.namenode.name.dir</name>
<value>file:/mnt/hadoop/namenode</value>
</property>
<property>
<name>dfs.datanode.data.dir</name>
<value>file:/mnt/hadoop/datanode</value>
</property>
</configuration>
" > hadoop-3.3.0/etc/hadoop/hdfs-site.xml

echo -e "<?xml version=\"1.0\"?>
<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>
<\x21-- Put site-specific property overrides in this file. -->
<configuration>
<\x21-- Site specific YARN configuration properties -->
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
<description>Tell NodeManagers that there will be an auxiliary
service called mapreduce.shuffle
that they need to implement
</description>
</property>
<property>
<name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
<value>org.apache.hadoop.mapred.ShuffleHandler</value>
<description>A class name as a means to implement the service
</description>
</property>
<property>
<name>yarn.resourcemanager.hostname</name>
<value>${MASTER}</value>
</property>
</configuration>
" > hadoop-3.3.0/etc/hadoop/yarn-site.xml


echo -e "<?xml version=\"1.0\"?>
<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>
<\x21-- Put site-specific property overrides in this file. -->
<configuration>
<\x21-- Site specific YARN configuration properties -->
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
<description>Use yarn to tell MapReduce that it will run as a YARN application
</description>
</property>
<property>
<name>yarn.app.mapreduce.am.env</name>
<value>HADOOP_MAPRED_HOME=/opt/hadoop-3.3.0/</value>
</property>
<property>
<name>mapreduce.map.env</name>
<value>HADOOP_MAPRED_HOME=/opt/hadoop-3.3.0/</value>
</property>
<property>
<name>mapreduce.reduce.env</name>
<value>HADOOP_MAPRED_HOME=/opt/hadoop-3.3.0/</value>
</property>
</configuration>
" > hadoop-3.3.0/etc/hadoop/mapred-site.xml

rm hadoop-3.3.0/etc/hadoop/workers
for ip in ${WORKERS}; do echo -e "${ip}" >> hadoop-3.3.0/etc/hadoop/workers ; done

tar czvf hadoop-3.3.0.tgz hadoop-3.3.0
for h in $WORKERS ; do
scp hadoop-3.3.0.tgz $h:.;
done;
cp hadoop-3.3.0.tgz ~/

(for all nodes)
cd
tar zxvf hadoop-3.3.0.tgz
sudo mv hadoop-3.3.0 /opt/

(for datanodes)
sudo mkdir -p /mnt/hadoop/datanode/
sudo chown -R hadoop:hadoop /mnt/hadoop/datanode/

(for namenode)
sudo mkdir -p /mnt/hadoop/namenode/hadoop-${USER}
sudo chown -R hadoop:hadoop /mnt/hadoop/namenode
/opt/hadoop-3.3.0/bin/hdfs namenode -format

/opt/hadoop-3.3.0/sbin/start-dfs.sh && /opt/hadoop-3.3.0/sbin/start-yarn.sh


(all nodes? for sqoop)
cd ~/download
wget https://apachemirror.sg.wuchna.com/sqoop/1.4.7/sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz
tar zxvf sqoop-1.4.7.bin__hadoop-2.6.0.tar.gz
cp sqoop-1.4.7.bin__hadoop-2.6.0/conf/sqoop-env-template.sh \
sqoop-1.4.7.bin__hadoop-2.6.0/conf/sqoop-env.sh
export HD="\/opt\/hadoop-3.3.0"
sed -i "s/#export HADOOP_COMMON_HOME=.*/export HADOOP_COMMON_HOME=${HD}/g" \ sqoop-1.4.7.bin__hadoop-2.6.0/conf/sqoop-env.sh
sed -i "s/#export HADOOP_MAPRED_HOME=.*/export HADOOP_MAPRED_HOME=${HD}/g" \ sqoop-1.4.7.bin__hadoop-2.6.0/conf/sqoop-env.sh
wget https://repo1.maven.org/maven2/commons-lang/commons-lang/2.6/commons-lang-2.6.jar
cp commons-lang-2.6.jar sqoop-1.4.7.bin__hadoop-2.6.0/lib/
sudo cp -rf sqoop-1.4.7.bin__hadoop-2.6.0 /opt/sqoop-1.4.7
sudo apt install libmysql-java
sudo ln -snvf /usr/share/java/mysql-connector-java.jar \
/opt/sqoop-1.4.7/lib/mysql-connector-java.jar
